{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import pandas as pd\n",
    "from glob import iglob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import load_model\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras import regularizers\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense, Lambda, LSTM, Dropout, RepeatVector, TimeDistributed\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.metrics import recall_score, accuracy_score, precision_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "benign=pd.read_csv(\"Thermostat/benign_traffic.csv\")\n",
    "x_train, x_opt, x_test = np.split(benign.sample(frac=1, random_state=20), [int(1/3*len(benign)), int(2/3*len(benign))])\n",
    "\n",
    "df_mirai = pd.concat((pd.read_csv(f) for f in iglob('Thermostat/mirai/*.csv', recursive=False)), ignore_index=True)\n",
    "df_gafgyt = pd.concat((pd.read_csv(f) for f in iglob('Thermostat/gafgyt/*.csv', recursive=False)), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_attack = df_mirai.append(df_gafgyt)\n",
    "df_attack['class'] = 'malicious'\n",
    "\n",
    "df_ben = x_train.copy()\n",
    "df_ben['class'] = 'benign'\n",
    "\n",
    "df_ben = df_ben.append(df_attack.sample(n=df_ben.shape[0], random_state=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting samples of benign and malicious data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_n = 2000\n",
    "atk = df_attack.sample(n=plot_n, random_state=76)\n",
    "nrm = x_train.sample(n=plot_n, random_state=42)\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "ax1.scatter(nrm['MI_dir_L0.1_weight'],nrm['MI_dir_L1_weight'],10,c='blue', label='normal',alpha=0.5)\n",
    "ax1.scatter(atk['MI_dir_L0.1_weight'],atk['MI_dir_L1_weight'],10,c='red',label='attack',alpha=0.5)\n",
    "\n",
    "plt.xlabel('MI_dir_L0.1_weight')\n",
    "plt.ylabel('MI_dir_L1_weight')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'feature': 'MI_dir_L0.1_weight', 'score': 1.0478924858805367},\n",
       " {'feature': 'H_L0.1_weight', 'score': 1.0478924689266897},\n",
       " {'feature': 'MI_dir_L1_weight', 'score': 1.040497174164095},\n",
       " {'feature': 'H_L1_weight', 'score': 1.040497174164095},\n",
       " {'feature': 'MI_dir_L3_weight', 'score': 1.014610037336972}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = ['benign','malicious']\n",
    "scored = []\n",
    "indices = {}\n",
    "shps = {}\n",
    "\n",
    "#classifying benign as true and attack as false\n",
    "for cl in classes:\n",
    "    indices[cl] = df_ben['class'] == cl    \n",
    "    shps[cl] =  df_ben[indices[cl]].shape[0]\n",
    "        \n",
    "for col in df_ben.columns:\n",
    "    if col == 'class':\n",
    "        continue\n",
    "    num = 0\n",
    "    den = 0\n",
    "    m = df_ben[col].mean()\n",
    "    \n",
    "    for cl in classes:\n",
    "        num += (shps[cl] / df_ben.shape[0]) * (m - df_ben[indices[cl]][col].mean())**2\n",
    "        den += (shps[cl] / df_ben.shape[0]) * df_ben[indices[cl]][col].var()\n",
    "    score = {'feature': col, 'score': num / den}\n",
    "    scored.append(score)\n",
    "    #print(score)\n",
    "scored.sort(key=lambda x: x['score'], reverse=True)\n",
    "scored[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnomalyModel:\n",
    "    #initialize model\n",
    "    def __init__(self, model, threshold, scaler):\n",
    "        self.model = model\n",
    "        self.threshold = threshold\n",
    "        self.scaler = scaler\n",
    "\n",
    "    #predict the outcome using treshold\n",
    "    def predict(self, x):\n",
    "        x_pred = self.model.predict(x)\n",
    "        mse = np.mean(np.power(x - x_pred, 2), axis=1)\n",
    "        y_pred = mse > self.threshold\n",
    "        return y_pred.astype(int)\n",
    "    \n",
    "    #scale the classes\n",
    "    def scale_predict_classes(self, x):\n",
    "        x = self.scaler.transform(x)\n",
    "        y_pred = self.predict(x)\n",
    "        classes_arr = []\n",
    "        for e in y_pred:\n",
    "            el = [0,0]\n",
    "            el[e] = 1\n",
    "            classes_arr.append(el)\n",
    "        print(classes_arr)\n",
    "\n",
    "        return np.array(classes_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model creation - Deep autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_model(input_dim):\n",
    "    inp = Input(shape=(input_dim,))\n",
    "    encoder = Dense(int(math.ceil(0.75 * input_dim)), activation=\"tanh\")(inp)\n",
    "    encoder = Dense(int(math.ceil(0.5 * input_dim)), activation=\"tanh\")(encoder)\n",
    "    encoder = Dense(int(math.ceil(0.25 * input_dim)), activation=\"tanh\")(encoder)\n",
    "    decoder = Dense(int(math.ceil(0.5 * input_dim)), activation=\"tanh\")(encoder)\n",
    "    decoder = Dense(int(math.ceil(0.75 * input_dim)), activation=\"tanh\")(decoder)\n",
    "    decoder = Dense(input_dim)(decoder)\n",
    "    return Model(inp, decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "137/137 [==============================] - 11s 56ms/step - loss: 0.5833 - val_loss: 0.2574\n",
      "Epoch 2/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.2000 - val_loss: 0.1942\n",
      "Epoch 3/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.1541 - val_loss: 0.1654\n",
      "Epoch 4/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.1179 - val_loss: 0.1433\n",
      "Epoch 5/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.1114 - val_loss: 0.1277\n",
      "Epoch 6/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.1254 - val_loss: 0.1153\n",
      "Epoch 7/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.0937 - val_loss: 0.1020\n",
      "Epoch 8/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.0767 - val_loss: 0.0940\n",
      "Epoch 9/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.0752 - val_loss: 0.0861\n",
      "Epoch 10/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.0612 - val_loss: 0.0830\n",
      "Epoch 11/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.0525 - val_loss: 0.0795\n",
      "Epoch 12/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.0511 - val_loss: 0.0733\n",
      "Epoch 13/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.0561 - val_loss: 0.0693\n",
      "Epoch 14/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.0515 - val_loss: 0.0680\n",
      "Epoch 15/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.0552 - val_loss: 0.0635\n",
      "Epoch 16/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.0468 - val_loss: 0.0608\n",
      "Epoch 17/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.0413 - val_loss: 0.0589\n",
      "Epoch 18/100\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.0451 - val_loss: 0.0579\n",
      "Epoch 19/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.0384 - val_loss: 0.0542\n",
      "Epoch 20/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.0384 - val_loss: 0.0529\n",
      "Epoch 21/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.0357 - val_loss: 0.0527\n",
      "Epoch 22/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.0351 - val_loss: 0.0516\n",
      "Epoch 23/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.0277 - val_loss: 0.0501\n",
      "Epoch 24/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.0307 - val_loss: 0.0483\n",
      "Epoch 25/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.0313 - val_loss: 0.0481\n",
      "Epoch 26/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.0347 - val_loss: 0.0488\n",
      "Epoch 27/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.0284 - val_loss: 0.0447\n",
      "Epoch 28/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.0299 - val_loss: 0.0441\n",
      "Epoch 29/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.0263 - val_loss: 0.0428\n",
      "Epoch 30/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.0240 - val_loss: 0.0411\n",
      "Epoch 31/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.0240 - val_loss: 0.0436\n",
      "Epoch 32/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.0235 - val_loss: 0.0398\n",
      "Epoch 33/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.0239 - val_loss: 0.0399\n",
      "Epoch 34/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.0234 - val_loss: 0.0395\n",
      "Epoch 35/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.0238 - val_loss: 0.0384\n",
      "Epoch 36/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.0215 - val_loss: 0.0392\n",
      "Epoch 37/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.0209 - val_loss: 0.0405\n",
      "Epoch 38/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.0224 - val_loss: 0.0372\n",
      "Epoch 39/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.0235 - val_loss: 0.0373\n",
      "Epoch 40/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.0226 - val_loss: 0.0368\n",
      "Epoch 41/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.0207 - val_loss: 0.0383\n",
      "Epoch 42/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.0205 - val_loss: 0.0351\n",
      "Epoch 43/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.0213 - val_loss: 0.0394\n",
      "Epoch 44/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.0204 - val_loss: 0.0362\n",
      "Epoch 45/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.0219 - val_loss: 0.0358\n",
      "Epoch 46/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.0168 - val_loss: 0.0342\n",
      "Epoch 47/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.0184 - val_loss: 0.0376\n",
      "Epoch 48/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.0173 - val_loss: 0.0351\n",
      "Epoch 49/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.0235 - val_loss: 0.0352\n",
      "Epoch 50/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.0197 - val_loss: 0.0352\n",
      "Epoch 51/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.0163 - val_loss: 0.0334\n",
      "Epoch 52/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.0175 - val_loss: 0.0352\n",
      "Epoch 53/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.0167 - val_loss: 0.0353\n",
      "Epoch 54/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.0191 - val_loss: 0.0352\n",
      "Epoch 55/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.0198 - val_loss: 0.0338\n",
      "Epoch 56/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.0184 - val_loss: 0.0337\n",
      "time: 46.855249881744385\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 115)]             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 87)                10092     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 58)                5104      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 29)                1711      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 58)                1740      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 87)                5133      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 115)               10120     \n",
      "=================================================================\n",
      "Total params: 33,900\n",
      "Trainable params: 33,900\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "top_n_features=115\n",
    "fs = [it['feature'] for it in scored[:top_n_features]]\n",
    "X_train = x_train[fs]\n",
    "X_opt = x_opt[fs]\n",
    "X_test = x_test[fs]\n",
    "    \n",
    "#standardize train and optimize data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_opt = scaler.transform(X_opt)\n",
    "    \n",
    "#call function to create the model\n",
    "model = deep_model(top_n_features)\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=\"adam\")\n",
    "cp = ModelCheckpoint(filepath=f\"anomaly/anomaly{top_n_features}.h5\",monitor='val_loss',save_best_only=True,verbose=0)\n",
    "\n",
    "#Early Stopping function until val_loss is same for 5 epochs\n",
    "es = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "start = time.time()  #time function to determine time taken by model\n",
    "epochs = 100\n",
    "    \n",
    "#fit on train dataset\n",
    "history = model.fit(X_train, X_train,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=(X_opt, X_opt),\n",
    "                    verbose=1,\n",
    "                    callbacks=[cp, es])\n",
    "    \n",
    "end = time.time()\n",
    "print('time:', end - start)\n",
    "print(model.summary())    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting n------------------\n",
      "For n 1\n",
      "Accuracy\n",
      "0.8709677419354839\n",
      "Precision 0.9827329562369753\n",
      "Recall 0.7552047586364676\n",
      "Confusion Matrix [[4313   58]\n",
      " [1070 3301]]\n",
      "For n 2\n",
      "Accuracy\n",
      "0.8732555479295355\n",
      "Precision 0.9886193471099132\n",
      "Recall 0.7552047586364676\n",
      "Confusion Matrix [[4333   38]\n",
      " [1070 3301]]\n",
      "For n 3\n",
      "Accuracy\n",
      "0.8742850606268588\n",
      "Precision 0.9915865384615384\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4343   28]\n",
      " [1071 3300]]\n",
      "For n 4\n",
      "Accuracy\n",
      "0.8750857927247769\n",
      "Precision 0.993676603432701\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4350   21]\n",
      " [1071 3300]]\n",
      "For n 5\n",
      "Accuracy\n",
      "0.8754289636238847\n",
      "Precision 0.9945750452079566\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4353   18]\n",
      " [1071 3300]]\n",
      "For n 6\n",
      "Accuracy\n",
      "0.8760009151223976\n",
      "Precision 0.9960760639903411\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4358   13]\n",
      " [1071 3300]]\n",
      "For n 7\n",
      "Accuracy\n",
      "0.8761153054221003\n",
      "Precision 0.9963768115942029\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4359   12]\n",
      " [1071 3300]]\n",
      "For n 8\n",
      "Accuracy\n",
      "0.8763440860215054\n",
      "Precision 0.9969788519637462\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4361   10]\n",
      " [1071 3300]]\n",
      "For n 9\n",
      "Accuracy\n",
      "0.876458476321208\n",
      "Precision 0.9972801450589301\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4362    9]\n",
      " [1071 3300]]\n",
      "For n 10\n",
      "Accuracy\n",
      "0.8765728666209105\n",
      "Precision 0.9975816203143894\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4363    8]\n",
      " [1071 3300]]\n",
      "For n 11\n",
      "Accuracy\n",
      "0.8765728666209105\n",
      "Precision 0.9975816203143894\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4363    8]\n",
      " [1071 3300]]\n",
      "For n 12\n",
      "Accuracy\n",
      "0.8765728666209105\n",
      "Precision 0.9975816203143894\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4363    8]\n",
      " [1071 3300]]\n",
      "For n 13\n",
      "Accuracy\n",
      "0.8765728666209105\n",
      "Precision 0.9975816203143894\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4363    8]\n",
      " [1071 3300]]\n",
      "For n 14\n",
      "Accuracy\n",
      "0.8765728666209105\n",
      "Precision 0.9975816203143894\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4363    8]\n",
      " [1071 3300]]\n",
      "For n 15\n",
      "Accuracy\n",
      "0.8765728666209105\n",
      "Precision 0.9975816203143894\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4363    8]\n",
      " [1071 3300]]\n",
      "For n 16\n",
      "Accuracy\n",
      "0.8766872569206131\n",
      "Precision 0.9978832778953735\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4364    7]\n",
      " [1071 3300]]\n",
      "For n 17\n",
      "Accuracy\n",
      "0.8766872569206131\n",
      "Precision 0.9978832778953735\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4364    7]\n",
      " [1071 3300]]\n",
      "For n 18\n",
      "Accuracy\n",
      "0.8768016472203157\n",
      "Precision 0.9981851179673321\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4365    6]\n",
      " [1071 3300]]\n",
      "For n 19\n",
      "Accuracy\n",
      "0.8770304278197208\n",
      "Precision 0.9987893462469734\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4367    4]\n",
      " [1071 3300]]\n",
      "For n 20\n",
      "Accuracy\n",
      "0.8770304278197208\n",
      "Precision 0.9987893462469734\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4367    4]\n",
      " [1071 3300]]\n",
      "For n 21\n",
      "Accuracy\n",
      "0.8770304278197208\n",
      "Precision 0.9987893462469734\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4367    4]\n",
      " [1071 3300]]\n",
      "For n 22\n",
      "Accuracy\n",
      "0.8771448181194235\n",
      "Precision 0.9990917347865577\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4368    3]\n",
      " [1071 3300]]\n",
      "For n 23\n",
      "Accuracy\n",
      "0.8772592084191261\n",
      "Precision 0.9993943064809206\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4369    2]\n",
      " [1071 3300]]\n",
      "For n 24\n",
      "Accuracy\n",
      "0.8772592084191261\n",
      "Precision 0.9993943064809206\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4369    2]\n",
      " [1071 3300]]\n",
      "For n 25\n",
      "Accuracy\n",
      "0.8773735987188287\n",
      "Precision 0.9996970614965162\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4370    1]\n",
      " [1071 3300]]\n",
      "For n 26\n",
      "Accuracy\n",
      "0.8773735987188287\n",
      "Precision 0.9996970614965162\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4370    1]\n",
      " [1071 3300]]\n",
      "For n 27\n",
      "Accuracy\n",
      "0.8773735987188287\n",
      "Precision 0.9996970614965162\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4370    1]\n",
      " [1071 3300]]\n",
      "For n 28\n",
      "Accuracy\n",
      "0.8774879890185312\n",
      "Precision 1.0\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4371    0]\n",
      " [1071 3300]]\n",
      "For n 29\n",
      "Accuracy\n",
      "0.8774879890185312\n",
      "Precision 1.0\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4371    0]\n",
      " [1071 3300]]\n"
     ]
    }
   ],
   "source": [
    "#use optimization dataset to predict and find mse for anomaly detection\n",
    "x_opt_predictions = model.predict(X_opt)\n",
    "mse = np.mean(np.power(X_opt - x_opt_predictions, 2), axis=1)\n",
    "mean = mse.mean()\n",
    "std = mse.std()\n",
    "    \n",
    "#sampling a benign dataset and adding a class malicious with 0\n",
    "df_benign = pd.DataFrame(x_opt[fs], columns=fs)\n",
    "df_benign['malicious'] = 0\n",
    "    \n",
    "#sampling a malicious dataset and adding a class malicious with 1\n",
    "df_malicious = df_attack.sample(n=df_benign.shape[0], random_state=39)[fs]\n",
    "df_malicious['malicious'] = 1\n",
    "    \n",
    "#combining benign and malicous dataset\n",
    "df_combo = df_benign.append(df_malicious)\n",
    "    \n",
    "#X_opt with feature values\n",
    "X_opt = df_combo.drop(columns=['malicious']).values\n",
    "X_opt_scaled = scaler.transform(X_opt)\n",
    "\n",
    "#Y_opt with target values\n",
    "Y_opt = df_combo['malicious']\n",
    "best_acc = 0\n",
    "best_n = 0\n",
    "#varying treshold and selecting the best model\n",
    "print('Selecting n------------------')\n",
    "for n in range(1,30):\n",
    "    tr = mean + n * std\n",
    "    m = AnomalyModel(model , tr, scaler)\n",
    "    Y_pred = m.predict(X_opt_scaled)\n",
    "    print(f'For n {n}')\n",
    "    print('Accuracy')\n",
    "    acc = accuracy_score(Y_opt, Y_pred)\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc  #determining the best aaccuracy\n",
    "        best_n = n\n",
    "    print(acc)\n",
    "    print('Precision',precision_score(Y_opt, Y_pred))\n",
    "    print(\"Recall\",recall_score(Y_opt, Y_pred))\n",
    "    print('Confusion Matrix',confusion_matrix(Y_opt, Y_pred))\n",
    "        \n",
    "        #accs.append({'acc': acc, 'n': top_n_features, 'cm': confusion_matrix(Y_test, Y_pred)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test-----------------\n",
      "best n 28\n",
      "Threshold  7.645269247916332\n",
      "Accuracy  0.880576527110501\n",
      "Precision  1.0\n",
      "Recall 0.7611530542210021\n",
      "Confusion Matrix [[4371    0]\n",
      " [1044 3327]]\n"
     ]
    }
   ],
   "source": [
    "#Creating a test Data frame and training model with test data\n",
    "df_benign = pd.DataFrame(X_test, columns=fs)\n",
    "df_benign['malicious'] = 0\n",
    "df_malicious = df_attack.sample(n=df_benign.shape[0], random_state=17)[fs]\n",
    "df_malicious['malicious'] = 1\n",
    "df2 = df_benign.append(df_malicious)\n",
    "X_test = df2.drop(columns=['malicious']).values\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "Y_test = df2['malicious']\n",
    "    \n",
    "#using best n value for treshold determined using opotimization dataset\n",
    "deep_tr = mean + best_n * std\n",
    "m = AnomalyModel(model , tr, scaler)\n",
    "Y_pred = m.predict(X_test_scaled)\n",
    "print('Test-----------------')\n",
    "print(f'best n {best_n}')\n",
    "print(\"Threshold \",tr)\n",
    "deep_acc = accuracy_score(Y_test, Y_pred)\n",
    "deep_precision = precision_score(Y_test, Y_pred)\n",
    "deep_recall = recall_score(Y_test, Y_pred)\n",
    "deep_cm = confusion_matrix(Y_test, Y_pred)\n",
    "print('Accuracy ',deep_acc)\n",
    "print('Precision ',deep_precision)\n",
    "print(\"Recall\",deep_recall)\n",
    "print('Confusion Matrix',deep_cm)\n",
    "   \n",
    "#print(accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_model(input_dim):\n",
    "    original_dim = input_dim\n",
    "    intermediate_dim = 64\n",
    "    latent_dim = 2\n",
    "\n",
    "    inputs = Input(shape=(original_dim,))\n",
    "    h = Dense(intermediate_dim, activation='relu')(inputs)\n",
    "    z_mean = Dense(latent_dim)(h)\n",
    "    z_log_sigma = Dense(latent_dim)(h)\n",
    "    \n",
    "    from keras import backend as K\n",
    "\n",
    "    def sampling(args):\n",
    "        z_mean, z_log_sigma = args\n",
    "        epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim),\n",
    "                              mean=0., stddev=0.1)\n",
    "        return z_mean + K.exp(z_log_sigma) * epsilon\n",
    "\n",
    "    z = Lambda(sampling)([z_mean, z_log_sigma])\n",
    "\n",
    "    # Create encoder\n",
    "    encoder = Model(inputs, [z_mean, z_log_sigma, z], name='encoder')\n",
    "\n",
    "    # Create decoder\n",
    "    latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
    "    x = Dense(intermediate_dim, activation='relu')(latent_inputs)\n",
    "    outputs = Dense(original_dim, activation='sigmoid')(x)\n",
    "    decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "\n",
    "    # instantiate VAE model\n",
    "    outputs = decoder(encoder(inputs)[2])\n",
    "    return Model(inputs, outputs, name='vae_mlp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "137/137 [==============================] - 3s 8ms/step - loss: 1.0536 - val_loss: 0.7376\n",
      "Epoch 2/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.7332 - val_loss: 0.7269\n",
      "Epoch 3/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.7446 - val_loss: 0.7226\n",
      "Epoch 4/100\n",
      "137/137 [==============================] - 0s 4ms/step - loss: 0.7107 - val_loss: 0.7203\n",
      "Epoch 5/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.6737 - val_loss: 0.7164\n",
      "Epoch 6/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.6675 - val_loss: 0.7147\n",
      "Epoch 7/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.7026 - val_loss: 0.7129\n",
      "Epoch 8/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.7091 - val_loss: 0.7115\n",
      "Epoch 9/100\n",
      "137/137 [==============================] - 0s 4ms/step - loss: 0.6898 - val_loss: 0.7106\n",
      "Epoch 10/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.7259 - val_loss: 0.7101\n",
      "Epoch 11/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.7030 - val_loss: 0.7084\n",
      "Epoch 12/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.7288 - val_loss: 0.7077\n",
      "Epoch 13/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.7076 - val_loss: 0.7072\n",
      "Epoch 14/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.6830 - val_loss: 0.7068\n",
      "Epoch 15/100\n",
      "137/137 [==============================] - 0s 4ms/step - loss: 0.6806 - val_loss: 0.7062\n",
      "Epoch 16/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.7354 - val_loss: 0.7056\n",
      "Epoch 17/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.6879 - val_loss: 0.7048\n",
      "Epoch 18/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.6936 - val_loss: 0.7043\n",
      "Epoch 19/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.6746 - val_loss: 0.7038\n",
      "Epoch 20/100\n",
      "137/137 [==============================] - 0s 4ms/step - loss: 0.7252 - val_loss: 0.7033\n",
      "Epoch 21/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.7058 - val_loss: 0.7031\n",
      "Epoch 22/100\n",
      "137/137 [==============================] - 0s 4ms/step - loss: 0.6939 - val_loss: 0.7029\n",
      "Epoch 23/100\n",
      "137/137 [==============================] - 0s 4ms/step - loss: 0.6803 - val_loss: 0.7027\n",
      "Epoch 24/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.7088 - val_loss: 0.7022\n",
      "Epoch 25/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.7546 - val_loss: 0.7017\n",
      "Epoch 26/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.6754 - val_loss: 0.7014\n",
      "Epoch 27/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.6805 - val_loss: 0.7013\n",
      "Epoch 28/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.7070 - val_loss: 0.7011\n",
      "Epoch 29/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.6802 - val_loss: 0.7010\n",
      "Epoch 30/100\n",
      "137/137 [==============================] - 0s 4ms/step - loss: 0.7074 - val_loss: 0.7005\n",
      "Epoch 31/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.6550 - val_loss: 0.7001\n",
      "Epoch 32/100\n",
      "137/137 [==============================] - 0s 4ms/step - loss: 0.6679 - val_loss: 0.6999\n",
      "Epoch 33/100\n",
      "137/137 [==============================] - 0s 4ms/step - loss: 0.6598 - val_loss: 0.6997\n",
      "Epoch 34/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.7128 - val_loss: 0.6994\n",
      "Epoch 35/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.6542 - val_loss: 0.6991\n",
      "Epoch 36/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.6991 - val_loss: 0.6989\n",
      "Epoch 37/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.6778 - val_loss: 0.6987\n",
      "Epoch 38/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.7108 - val_loss: 0.6988\n",
      "Epoch 39/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.6667 - val_loss: 0.6983\n",
      "Epoch 40/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.7167 - val_loss: 0.6979\n",
      "Epoch 41/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.7050 - val_loss: 0.6981\n",
      "Epoch 42/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.6703 - val_loss: 0.6978\n",
      "Epoch 43/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.7196 - val_loss: 0.6978\n",
      "Epoch 44/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.6767 - val_loss: 0.6974\n",
      "Epoch 45/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.7323 - val_loss: 0.6973\n",
      "Epoch 46/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.7308 - val_loss: 0.6972\n",
      "Epoch 47/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.7036 - val_loss: 0.6969\n",
      "Epoch 48/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.6961 - val_loss: 0.6966\n",
      "Epoch 49/100\n",
      "137/137 [==============================] - 0s 4ms/step - loss: 0.6799 - val_loss: 0.6966\n",
      "Epoch 50/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.7018 - val_loss: 0.6971\n",
      "Epoch 51/100\n",
      "137/137 [==============================] - 0s 4ms/step - loss: 0.7012 - val_loss: 0.6967\n",
      "Epoch 52/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.6814 - val_loss: 0.6966\n",
      "Epoch 53/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.7063 - val_loss: 0.6965\n",
      "Epoch 54/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.6843 - val_loss: 0.6964\n",
      "Epoch 55/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.6864 - val_loss: 0.6967\n",
      "Epoch 56/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.6746 - val_loss: 0.6963\n",
      "Epoch 57/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.7075 - val_loss: 0.6965\n",
      "Epoch 58/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.7015 - val_loss: 0.6963\n",
      "Epoch 59/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.6971 - val_loss: 0.6962\n",
      "Epoch 60/100\n",
      "137/137 [==============================] - 0s 4ms/step - loss: 0.6731 - val_loss: 0.6962\n",
      "Epoch 61/100\n",
      "137/137 [==============================] - 0s 4ms/step - loss: 0.6730 - val_loss: 0.6961\n",
      "Epoch 62/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.7516 - val_loss: 0.6960\n",
      "Epoch 63/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.7392 - val_loss: 0.6961\n",
      "Epoch 64/100\n",
      "137/137 [==============================] - 0s 4ms/step - loss: 0.7075 - val_loss: 0.6955\n",
      "Epoch 65/100\n",
      "137/137 [==============================] - 0s 4ms/step - loss: 0.7027 - val_loss: 0.6958\n",
      "Epoch 66/100\n",
      "137/137 [==============================] - 0s 4ms/step - loss: 0.6993 - val_loss: 0.6959\n",
      "Epoch 67/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.6984 - val_loss: 0.6956\n",
      "Epoch 68/100\n",
      "137/137 [==============================] - 0s 4ms/step - loss: 0.6709 - val_loss: 0.6954\n",
      "Epoch 69/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.6430 - val_loss: 0.6954\n",
      "Epoch 70/100\n",
      "137/137 [==============================] - 0s 4ms/step - loss: 0.7233 - val_loss: 0.6953\n",
      "Epoch 71/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.6817 - val_loss: 0.6955\n",
      "Epoch 72/100\n",
      "137/137 [==============================] - 0s 4ms/step - loss: 0.6948 - val_loss: 0.6954\n",
      "Epoch 73/100\n",
      "137/137 [==============================] - 0s 4ms/step - loss: 0.7246 - val_loss: 0.6953\n",
      "Epoch 74/100\n",
      "137/137 [==============================] - 0s 4ms/step - loss: 0.6856 - val_loss: 0.6956\n",
      "Epoch 75/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.6522 - val_loss: 0.6951\n",
      "Epoch 76/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.6794 - val_loss: 0.6954\n",
      "Epoch 77/100\n",
      "137/137 [==============================] - 0s 4ms/step - loss: 0.6900 - val_loss: 0.6950\n",
      "Epoch 78/100\n",
      "137/137 [==============================] - 0s 4ms/step - loss: 0.6813 - val_loss: 0.6956\n",
      "Epoch 79/100\n",
      "137/137 [==============================] - 0s 4ms/step - loss: 0.6889 - val_loss: 0.6949\n",
      "Epoch 80/100\n",
      "137/137 [==============================] - 0s 4ms/step - loss: 0.6430 - val_loss: 0.6950\n",
      "Epoch 81/100\n",
      "137/137 [==============================] - 0s 4ms/step - loss: 0.7248 - val_loss: 0.6951\n",
      "Epoch 82/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.7029 - val_loss: 0.6950\n",
      "Epoch 83/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.6832 - val_loss: 0.6949\n",
      "Epoch 84/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.6451 - val_loss: 0.6950\n",
      "Epoch 85/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.6893 - val_loss: 0.6947\n",
      "Epoch 86/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.7069 - val_loss: 0.6948\n",
      "Epoch 87/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.6658 - val_loss: 0.6948\n",
      "Epoch 88/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.6929 - val_loss: 0.6949\n",
      "Epoch 89/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.7118 - val_loss: 0.6950\n",
      "Epoch 90/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.7289 - val_loss: 0.6947\n",
      "time: 50.14804291725159\n",
      "Model: \"vae_mlp\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 115)]             0         \n",
      "_________________________________________________________________\n",
      "encoder (Functional)         [(None, 2), (None, 2), (N 7684      \n",
      "_________________________________________________________________\n",
      "decoder (Functional)         (None, 115)               7667      \n",
      "=================================================================\n",
      "Total params: 15,351\n",
      "Trainable params: 15,351\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "top_n_features=115\n",
    "fs = [it['feature'] for it in scored[:top_n_features]]\n",
    "X_train = x_train[fs]\n",
    "X_opt = x_opt[fs]\n",
    "X_test = x_test[fs]\n",
    "    \n",
    "#standardize train and optimize data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_opt = scaler.transform(X_opt)\n",
    "    \n",
    "#call function to create the model\n",
    "model = vae_model(top_n_features)\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=\"adam\")\n",
    "cp = ModelCheckpoint(filepath=f\"anomaly/anomaly_vae.h5\",monitor='val_loss',\n",
    "                     save_best_only=True,save_weights_only=True,verbose=0)\n",
    "\n",
    "#Early Stopping function until val_loss is same for 5 epochs\n",
    "es = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "start = time.time()  #time function to determine time taken by model\n",
    "epochs = 100\n",
    "    \n",
    "#fit on train dataset\n",
    "history = model.fit(X_train, X_train,\n",
    "                    epochs=epochs, validation_data=(X_opt, X_opt),\n",
    "                    verbose=1, callbacks=[cp, es])\n",
    "    \n",
    "end = time.time()\n",
    "print('time:', end - start)\n",
    "print(model.summary()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting n------------------\n",
      "For n 1\n",
      "Accuracy\n",
      "0.9876458476321208\n",
      "Precision 0.9758874748827864\n",
      "Recall 1.0\n",
      "Confusion Matrix [[4263  108]\n",
      " [   0 4371]]\n",
      "For n 2\n",
      "Accuracy\n",
      "0.9932509723175474\n",
      "Precision 0.9869015356820234\n",
      "Recall 0.9997712194005949\n",
      "Confusion Matrix [[4313   58]\n",
      " [   1 4370]]\n",
      "For n 3\n",
      "Accuracy\n",
      "0.9958819492107069\n",
      "Precision 0.9920544835414302\n",
      "Recall 0.9997712194005949\n",
      "Confusion Matrix [[4336   35]\n",
      " [   1 4370]]\n",
      "For n 4\n",
      "Accuracy\n",
      "0.9962251201098147\n",
      "Precision 0.9927305770104498\n",
      "Recall 0.9997712194005949\n",
      "Confusion Matrix [[4339   32]\n",
      " [   1 4370]]\n",
      "For n 5\n",
      "Accuracy\n",
      "0.996682681308625\n",
      "Precision 0.9936334697589814\n",
      "Recall 0.9997712194005949\n",
      "Confusion Matrix [[4343   28]\n",
      " [   1 4370]]\n",
      "For n 6\n",
      "Accuracy\n",
      "0.9970258522077328\n",
      "Precision 0.9943117178612059\n",
      "Recall 0.9997712194005949\n",
      "Confusion Matrix [[4346   25]\n",
      " [   1 4370]]\n",
      "For n 7\n",
      "Accuracy\n",
      "0.997254632807138\n",
      "Precision 0.9947643979057592\n",
      "Recall 0.9997712194005949\n",
      "Confusion Matrix [[4348   23]\n",
      " [   1 4370]]\n",
      "For n 8\n",
      "Accuracy\n",
      "0.9978265843056509\n",
      "Precision 0.9961240310077519\n",
      "Recall 0.9995424388011896\n",
      "Confusion Matrix [[4354   17]\n",
      " [   2 4369]]\n",
      "For n 9\n",
      "Accuracy\n",
      "0.8763440860215054\n",
      "Precision 0.9969788519637462\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4361   10]\n",
      " [1071 3300]]\n",
      "For n 10\n",
      "Accuracy\n",
      "0.8763440860215054\n",
      "Precision 0.9969788519637462\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4361   10]\n",
      " [1071 3300]]\n",
      "For n 11\n",
      "Accuracy\n",
      "0.876458476321208\n",
      "Precision 0.9972801450589301\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4362    9]\n",
      " [1071 3300]]\n",
      "For n 12\n",
      "Accuracy\n",
      "0.8765728666209105\n",
      "Precision 0.9975816203143894\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4363    8]\n",
      " [1071 3300]]\n",
      "For n 13\n",
      "Accuracy\n",
      "0.8768016472203157\n",
      "Precision 0.9981851179673321\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4365    6]\n",
      " [1071 3300]]\n",
      "For n 14\n",
      "Accuracy\n",
      "0.8770304278197208\n",
      "Precision 0.9987893462469734\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4367    4]\n",
      " [1071 3300]]\n",
      "For n 15\n",
      "Accuracy\n",
      "0.8770304278197208\n",
      "Precision 0.9987893462469734\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4367    4]\n",
      " [1071 3300]]\n",
      "For n 16\n",
      "Accuracy\n",
      "0.8770304278197208\n",
      "Precision 0.9987893462469734\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4367    4]\n",
      " [1071 3300]]\n",
      "For n 17\n",
      "Accuracy\n",
      "0.8770304278197208\n",
      "Precision 0.9987893462469734\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4367    4]\n",
      " [1071 3300]]\n",
      "For n 18\n",
      "Accuracy\n",
      "0.8770304278197208\n",
      "Precision 0.9987893462469734\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4367    4]\n",
      " [1071 3300]]\n",
      "For n 19\n",
      "Accuracy\n",
      "0.8771448181194235\n",
      "Precision 0.9990917347865577\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4368    3]\n",
      " [1071 3300]]\n",
      "For n 20\n",
      "Accuracy\n",
      "0.8772592084191261\n",
      "Precision 0.9993943064809206\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4369    2]\n",
      " [1071 3300]]\n",
      "For n 21\n",
      "Accuracy\n",
      "0.8772592084191261\n",
      "Precision 0.9993943064809206\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4369    2]\n",
      " [1071 3300]]\n",
      "For n 22\n",
      "Accuracy\n",
      "0.8772592084191261\n",
      "Precision 0.9993943064809206\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4369    2]\n",
      " [1071 3300]]\n",
      "For n 23\n",
      "Accuracy\n",
      "0.8772592084191261\n",
      "Precision 0.9993943064809206\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4369    2]\n",
      " [1071 3300]]\n",
      "For n 24\n",
      "Accuracy\n",
      "0.8773735987188287\n",
      "Precision 0.9996970614965162\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4370    1]\n",
      " [1071 3300]]\n",
      "For n 25\n",
      "Accuracy\n",
      "0.8774879890185312\n",
      "Precision 1.0\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4371    0]\n",
      " [1071 3300]]\n",
      "For n 26\n",
      "Accuracy\n",
      "0.8774879890185312\n",
      "Precision 1.0\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4371    0]\n",
      " [1071 3300]]\n",
      "For n 27\n",
      "Accuracy\n",
      "0.8774879890185312\n",
      "Precision 1.0\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4371    0]\n",
      " [1071 3300]]\n",
      "For n 28\n",
      "Accuracy\n",
      "0.8774879890185312\n",
      "Precision 1.0\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4371    0]\n",
      " [1071 3300]]\n",
      "For n 29\n",
      "Accuracy\n",
      "0.8774879890185312\n",
      "Precision 1.0\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4371    0]\n",
      " [1071 3300]]\n"
     ]
    }
   ],
   "source": [
    "#use optimization dataset to predict and find mse for anomaly detection\n",
    "x_opt_predictions = model.predict(X_opt)\n",
    "mse = np.mean(np.power(X_opt - x_opt_predictions, 2), axis=1)\n",
    "mean = mse.mean()\n",
    "std = mse.std()\n",
    "    \n",
    "#sampling a benign dataset and adding a class malicious with 0\n",
    "df_benign = pd.DataFrame(x_opt[fs], columns=fs)\n",
    "df_benign['malicious'] = 0\n",
    "    \n",
    "#sampling a malicious dataset and adding a class malicious with 1\n",
    "df_malicious = df_attack.sample(n=df_benign.shape[0], random_state=39)[fs]\n",
    "df_malicious['malicious'] = 1\n",
    "    \n",
    "#combining benign and malicous dataset\n",
    "df_combo = df_benign.append(df_malicious)\n",
    "    \n",
    "#X_opt with feature values\n",
    "X_opt = df_combo.drop(columns=['malicious']).values\n",
    "X_opt_scaled = scaler.transform(X_opt)\n",
    "\n",
    "#Y_opt with target values\n",
    "Y_opt = df_combo['malicious']\n",
    "best_acc = 0\n",
    "best_n = 0\n",
    "#varying treshold and selecting the best model\n",
    "print('Selecting n------------------')\n",
    "for n in range(1,30):\n",
    "    tr = mean + n * std\n",
    "    m = AnomalyModel(model , tr, scaler)\n",
    "    Y_pred = m.predict(X_opt_scaled)\n",
    "    print(f'For n {n}')\n",
    "    print('Accuracy')\n",
    "    acc = accuracy_score(Y_opt, Y_pred)\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc  #determining the best aaccuracy\n",
    "        best_n = n\n",
    "    print(acc)\n",
    "    print('Precision',precision_score(Y_opt, Y_pred))\n",
    "    print(\"Recall\",recall_score(Y_opt, Y_pred))\n",
    "    print('Confusion Matrix',confusion_matrix(Y_opt, Y_pred))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test-----------------\n",
      "best n 8\n",
      "Threshold  16.592289127460308\n",
      "Accuracy  0.9983985358041638\n",
      "Precision  0.9968072976054732\n",
      "Recall 1.0\n",
      "Confusion Matrix [[4357   14]\n",
      " [   0 4371]]\n"
     ]
    }
   ],
   "source": [
    "#Creating a test Data frame and training model with test data\n",
    "df_benign = pd.DataFrame(X_test, columns=fs)\n",
    "df_benign['malicious'] = 0\n",
    "df_malicious = df_attack.sample(n=df_benign.shape[0], random_state=17)[fs]\n",
    "df_malicious['malicious'] = 1\n",
    "df2 = df_benign.append(df_malicious)\n",
    "X_test = df2.drop(columns=['malicious']).values\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "Y_test = df2['malicious']\n",
    "    \n",
    "#using best n value for treshold determined using opotimization dataset\n",
    "tr = mean + best_n * std\n",
    "m = AnomalyModel(model , tr, scaler)\n",
    "Y_pred = m.predict(X_test_scaled)\n",
    "print('Test-----------------')\n",
    "print(f'best n {best_n}')\n",
    "print(\"Threshold \",tr)\n",
    "vae_acc = accuracy_score(Y_test, Y_pred)\n",
    "vae_precision = precision_score(Y_test, Y_pred)\n",
    "vae_recall = recall_score(Y_test, Y_pred)\n",
    "vae_cm = confusion_matrix(Y_test, Y_pred)\n",
    "print('Accuracy ',vae_acc)\n",
    "print('Precision ',vae_precision)\n",
    "print(\"Recall\",vae_recall)\n",
    "print('Confusion Matrix',vae_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undercomplete Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uc_model(input_dim):\n",
    "    input_img = Input(shape=(input_dim,))\n",
    "    encoded = Dense(15, activation='relu')(input_img)\n",
    "    decoded = Dense(input_dim, activation='sigmoid')(encoded)\n",
    "\n",
    "    autoencoder = Model(input_img, decoded)\n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "137/137 [==============================] - 2s 6ms/step - loss: 1.1272 - val_loss: 0.7674\n",
      "Epoch 2/100\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.6932 - val_loss: 0.7297\n",
      "Epoch 3/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.7579 - val_loss: 0.7142\n",
      "Epoch 4/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.7166 - val_loss: 0.7044\n",
      "Epoch 5/100\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.7711 - val_loss: 0.6970\n",
      "Epoch 6/100\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.6681 - val_loss: 0.6911\n",
      "Epoch 7/100\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.6721 - val_loss: 0.6871\n",
      "Epoch 8/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.6735 - val_loss: 0.6835\n",
      "Epoch 9/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.6861 - val_loss: 0.6802\n",
      "Epoch 10/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.6836 - val_loss: 0.6776\n",
      "Epoch 11/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.6556 - val_loss: 0.6754\n",
      "Epoch 12/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.6971 - val_loss: 0.6733\n",
      "Epoch 13/100\n",
      "137/137 [==============================] - 0s 4ms/step - loss: 0.6771 - val_loss: 0.6715\n",
      "Epoch 14/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.7253 - val_loss: 0.6699\n",
      "Epoch 15/100\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.6977 - val_loss: 0.6682\n",
      "Epoch 16/100\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.6830 - val_loss: 0.6669\n",
      "Epoch 17/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.6309 - val_loss: 0.6658\n",
      "Epoch 18/100\n",
      "137/137 [==============================] - 0s 4ms/step - loss: 0.6866 - val_loss: 0.6652\n",
      "Epoch 19/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.6773 - val_loss: 0.6644\n",
      "Epoch 20/100\n",
      "137/137 [==============================] - 0s 4ms/step - loss: 0.6519 - val_loss: 0.6638\n",
      "Epoch 21/100\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.6728 - val_loss: 0.6633\n",
      "Epoch 22/100\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.6791 - val_loss: 0.6629\n",
      "Epoch 23/100\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.6589 - val_loss: 0.6624\n",
      "Epoch 24/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.6666 - val_loss: 0.6621\n",
      "Epoch 25/100\n",
      "137/137 [==============================] - 0s 4ms/step - loss: 0.6342 - val_loss: 0.6618\n",
      "Epoch 26/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.6358 - val_loss: 0.6615\n",
      "Epoch 27/100\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.6661 - val_loss: 0.6611\n",
      "Epoch 28/100\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.6330 - val_loss: 0.6610\n",
      "Epoch 29/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.6608 - val_loss: 0.6607\n",
      "Epoch 30/100\n",
      "137/137 [==============================] - 0s 4ms/step - loss: 0.6256 - val_loss: 0.6605\n",
      "Epoch 31/100\n",
      "137/137 [==============================] - 0s 4ms/step - loss: 0.6234 - val_loss: 0.6603\n",
      "Epoch 32/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.6680 - val_loss: 0.6601\n",
      "Epoch 33/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.6272 - val_loss: 0.6599\n",
      "Epoch 34/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.6604 - val_loss: 0.6598\n",
      "Epoch 35/100\n",
      "137/137 [==============================] - 0s 4ms/step - loss: 0.6602 - val_loss: 0.6596\n",
      "Epoch 36/100\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.6637 - val_loss: 0.6593\n",
      "Epoch 37/100\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.6276 - val_loss: 0.6594\n",
      "Epoch 38/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.6407 - val_loss: 0.6590\n",
      "Epoch 39/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.6569 - val_loss: 0.6589\n",
      "Epoch 40/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.6238 - val_loss: 0.6587\n",
      "Epoch 41/100\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.6556 - val_loss: 0.6588\n",
      "Epoch 42/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.6547 - val_loss: 0.6585\n",
      "Epoch 43/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.6444 - val_loss: 0.6584\n",
      "Epoch 44/100\n",
      "137/137 [==============================] - 1s 6ms/step - loss: 0.6432 - val_loss: 0.6583\n",
      "Epoch 45/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.6298 - val_loss: 0.6582\n",
      "Epoch 46/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.6616 - val_loss: 0.6582\n",
      "Epoch 47/100\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.6234 - val_loss: 0.6582\n",
      "Epoch 48/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.6582 - val_loss: 0.6580\n",
      "Epoch 49/100\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.6772 - val_loss: 0.6577\n",
      "Epoch 50/100\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.6576 - val_loss: 0.6576\n",
      "Epoch 51/100\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.6400 - val_loss: 0.6575\n",
      "Epoch 52/100\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.6473 - val_loss: 0.6574\n",
      "Epoch 53/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.6551 - val_loss: 0.6573\n",
      "Epoch 54/100\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.6406 - val_loss: 0.6571\n",
      "Epoch 55/100\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.6618 - val_loss: 0.6570\n",
      "Epoch 56/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.6835 - val_loss: 0.6570\n",
      "Epoch 57/100\n",
      "137/137 [==============================] - 1s 6ms/step - loss: 0.6388 - val_loss: 0.6569\n",
      "Epoch 58/100\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.6222 - val_loss: 0.6569\n",
      "Epoch 59/100\n",
      "137/137 [==============================] - 1s 6ms/step - loss: 0.6833 - val_loss: 0.6568\n",
      "Epoch 60/100\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.6335 - val_loss: 0.6568\n",
      "Epoch 61/100\n",
      "137/137 [==============================] - 0s 4ms/step - loss: 0.6735 - val_loss: 0.6568\n",
      "Epoch 62/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.6245 - val_loss: 0.6568\n",
      "Epoch 63/100\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.6789 - val_loss: 0.6566\n",
      "Epoch 64/100\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.6703 - val_loss: 0.6567\n",
      "Epoch 65/100\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.6362 - val_loss: 0.6566\n",
      "Epoch 66/100\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.6450 - val_loss: 0.6565\n",
      "Epoch 67/100\n",
      "137/137 [==============================] - 1s 6ms/step - loss: 0.6302 - val_loss: 0.6564\n",
      "Epoch 68/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.6676 - val_loss: 0.6565\n",
      "Epoch 69/100\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.6122 - val_loss: 0.6562\n",
      "Epoch 70/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.6514 - val_loss: 0.6563\n",
      "Epoch 71/100\n",
      "137/137 [==============================] - 0s 4ms/step - loss: 0.6942 - val_loss: 0.6564\n",
      "Epoch 72/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.6588 - val_loss: 0.6563\n",
      "Epoch 73/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.6330 - val_loss: 0.6562\n",
      "Epoch 74/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.6497 - val_loss: 0.6563\n",
      "Epoch 75/100\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.6812 - val_loss: 0.6561\n",
      "Epoch 76/100\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.6747 - val_loss: 0.6561\n",
      "Epoch 77/100\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.6774 - val_loss: 0.6560\n",
      "Epoch 78/100\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.6538 - val_loss: 0.6561\n",
      "Epoch 79/100\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.6583 - val_loss: 0.6560\n",
      "Epoch 80/100\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.6630 - val_loss: 0.6561\n",
      "Epoch 81/100\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.6589 - val_loss: 0.6560\n",
      "Epoch 82/100\n",
      "137/137 [==============================] - 0s 4ms/step - loss: 0.6417 - val_loss: 0.6561\n",
      "time: 47.87788653373718\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 115)]             0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 15)                1740      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 115)               1840      \n",
      "=================================================================\n",
      "Total params: 3,580\n",
      "Trainable params: 3,580\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "top_n_features=115\n",
    "fs = [it['feature'] for it in scored[:top_n_features]]\n",
    "X_train = x_train[fs]\n",
    "X_opt = x_opt[fs]\n",
    "X_test = x_test[fs]\n",
    "    \n",
    "#standardize train and optimize data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_opt = scaler.transform(X_opt)\n",
    "    \n",
    "#call function to create the model\n",
    "model = uc_model(top_n_features)\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=\"adam\")\n",
    "cp = ModelCheckpoint(filepath=f\"anomaly/anomaly_undercomplete.h5\",monitor='val_loss',\n",
    "                     save_best_only=True,save_weights_only=True,verbose=0)\n",
    "\n",
    "#Early Stopping function until val_loss is same for 5 epochs\n",
    "es = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "start = time.time()  #time function to determine time taken by model\n",
    "epochs = 100\n",
    "    \n",
    "#fit on train dataset\n",
    "history = model.fit(X_train, X_train,\n",
    "                    epochs=epochs, validation_data=(X_opt, X_opt),\n",
    "                    verbose=1, callbacks=[cp, es])\n",
    "    \n",
    "end = time.time()\n",
    "print('time:', end - start)\n",
    "print(model.summary()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting n------------------\n",
      "For n 1\n",
      "Accuracy\n",
      "0.9876458476321208\n",
      "Precision 0.9758874748827864\n",
      "Recall 1.0\n",
      "Confusion Matrix [[4263  108]\n",
      " [   0 4371]]\n",
      "For n 2\n",
      "Accuracy\n",
      "0.9930221917181423\n",
      "Precision 0.9864559819413092\n",
      "Recall 0.9997712194005949\n",
      "Confusion Matrix [[4311   60]\n",
      " [   1 4370]]\n",
      "For n 3\n",
      "Accuracy\n",
      "0.9956531686113017\n",
      "Precision 0.9916042659405492\n",
      "Recall 0.9997712194005949\n",
      "Confusion Matrix [[4334   37]\n",
      " [   1 4370]]\n",
      "For n 4\n",
      "Accuracy\n",
      "0.9963395104095173\n",
      "Precision 0.9929561463303794\n",
      "Recall 0.9997712194005949\n",
      "Confusion Matrix [[4340   31]\n",
      " [   1 4370]]\n",
      "For n 5\n",
      "Accuracy\n",
      "0.996682681308625\n",
      "Precision 0.9936334697589814\n",
      "Recall 0.9997712194005949\n",
      "Confusion Matrix [[4343   28]\n",
      " [   1 4370]]\n",
      "For n 6\n",
      "Accuracy\n",
      "0.9970258522077328\n",
      "Precision 0.9943117178612059\n",
      "Recall 0.9997712194005949\n",
      "Confusion Matrix [[4346   25]\n",
      " [   1 4370]]\n",
      "For n 7\n",
      "Accuracy\n",
      "0.9973690231068405\n",
      "Precision 0.9949908925318761\n",
      "Recall 0.9997712194005949\n",
      "Confusion Matrix [[4349   22]\n",
      " [   1 4370]]\n",
      "For n 8\n",
      "Accuracy\n",
      "0.8757721345229924\n",
      "Precision 0.995475113122172\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4356   15]\n",
      " [1071 3300]]\n",
      "For n 9\n",
      "Accuracy\n",
      "0.8763440860215054\n",
      "Precision 0.9969788519637462\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4361   10]\n",
      " [1071 3300]]\n",
      "For n 10\n",
      "Accuracy\n",
      "0.876458476321208\n",
      "Precision 0.9972801450589301\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4362    9]\n",
      " [1071 3300]]\n",
      "For n 11\n",
      "Accuracy\n",
      "0.876458476321208\n",
      "Precision 0.9972801450589301\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4362    9]\n",
      " [1071 3300]]\n",
      "For n 12\n",
      "Accuracy\n",
      "0.876458476321208\n",
      "Precision 0.9972801450589301\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4362    9]\n",
      " [1071 3300]]\n",
      "For n 13\n",
      "Accuracy\n",
      "0.8766872569206131\n",
      "Precision 0.9978832778953735\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4364    7]\n",
      " [1071 3300]]\n",
      "For n 14\n",
      "Accuracy\n",
      "0.8770304278197208\n",
      "Precision 0.9987893462469734\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4367    4]\n",
      " [1071 3300]]\n",
      "For n 15\n",
      "Accuracy\n",
      "0.8770304278197208\n",
      "Precision 0.9987893462469734\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4367    4]\n",
      " [1071 3300]]\n",
      "For n 16\n",
      "Accuracy\n",
      "0.8770304278197208\n",
      "Precision 0.9987893462469734\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4367    4]\n",
      " [1071 3300]]\n",
      "For n 17\n",
      "Accuracy\n",
      "0.8770304278197208\n",
      "Precision 0.9987893462469734\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4367    4]\n",
      " [1071 3300]]\n",
      "For n 18\n",
      "Accuracy\n",
      "0.8771448181194235\n",
      "Precision 0.9990917347865577\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4368    3]\n",
      " [1071 3300]]\n",
      "For n 19\n",
      "Accuracy\n",
      "0.8772592084191261\n",
      "Precision 0.9993943064809206\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4369    2]\n",
      " [1071 3300]]\n",
      "For n 20\n",
      "Accuracy\n",
      "0.8772592084191261\n",
      "Precision 0.9993943064809206\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4369    2]\n",
      " [1071 3300]]\n",
      "For n 21\n",
      "Accuracy\n",
      "0.8772592084191261\n",
      "Precision 0.9993943064809206\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4369    2]\n",
      " [1071 3300]]\n",
      "For n 22\n",
      "Accuracy\n",
      "0.8772592084191261\n",
      "Precision 0.9993943064809206\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4369    2]\n",
      " [1071 3300]]\n",
      "For n 23\n",
      "Accuracy\n",
      "0.8772592084191261\n",
      "Precision 0.9993943064809206\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4369    2]\n",
      " [1071 3300]]\n",
      "For n 24\n",
      "Accuracy\n",
      "0.8774879890185312\n",
      "Precision 1.0\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4371    0]\n",
      " [1071 3300]]\n",
      "For n 25\n",
      "Accuracy\n",
      "0.8774879890185312\n",
      "Precision 1.0\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4371    0]\n",
      " [1071 3300]]\n",
      "For n 26\n",
      "Accuracy\n",
      "0.8774879890185312\n",
      "Precision 1.0\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4371    0]\n",
      " [1071 3300]]\n",
      "For n 27\n",
      "Accuracy\n",
      "0.8774879890185312\n",
      "Precision 1.0\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4371    0]\n",
      " [1071 3300]]\n",
      "For n 28\n",
      "Accuracy\n",
      "0.8774879890185312\n",
      "Precision 1.0\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4371    0]\n",
      " [1071 3300]]\n",
      "For n 29\n",
      "Accuracy\n",
      "0.8774879890185312\n",
      "Precision 1.0\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4371    0]\n",
      " [1071 3300]]\n"
     ]
    }
   ],
   "source": [
    "#use optimization dataset to predict and find mse for anomaly detection\n",
    "x_opt_predictions = model.predict(X_opt)\n",
    "mse = np.mean(np.power(X_opt - x_opt_predictions, 2), axis=1)\n",
    "mean = mse.mean()\n",
    "std = mse.std()\n",
    "    \n",
    "#sampling a benign dataset and adding a class malicious with 0\n",
    "df_benign = pd.DataFrame(x_opt[fs], columns=fs)\n",
    "df_benign['malicious'] = 0\n",
    "    \n",
    "#sampling a malicious dataset and adding a class malicious with 1\n",
    "df_malicious = df_attack.sample(n=df_benign.shape[0], random_state=39)[fs]\n",
    "df_malicious['malicious'] = 1\n",
    "    \n",
    "#combining benign and malicous dataset\n",
    "df_combo = df_benign.append(df_malicious)\n",
    "    \n",
    "#X_opt with feature values\n",
    "X_opt = df_combo.drop(columns=['malicious']).values\n",
    "X_opt_scaled = scaler.transform(X_opt)\n",
    "\n",
    "#Y_opt with target values\n",
    "Y_opt = df_combo['malicious']\n",
    "best_acc = 0\n",
    "best_n = 0\n",
    "#varying treshold and selecting the best model\n",
    "print('Selecting n------------------')\n",
    "for n in range(1,30):\n",
    "    tr = mean + n * std\n",
    "    m = AnomalyModel(model , tr, scaler)\n",
    "    Y_pred = m.predict(X_opt_scaled)\n",
    "    print(f'For n {n}')\n",
    "    print('Accuracy')\n",
    "    acc = accuracy_score(Y_opt, Y_pred)\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc  #determining the best aaccuracy\n",
    "        best_n = n\n",
    "    print(acc)\n",
    "    print('Precision',precision_score(Y_opt, Y_pred))\n",
    "    print(\"Recall\",recall_score(Y_opt, Y_pred))\n",
    "    print('Confusion Matrix',confusion_matrix(Y_opt, Y_pred))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test-----------------\n",
      "best n 7\n",
      "Threshold  13.91239174152284\n",
      "Accuracy  0.9978265843056509\n",
      "Precision  0.9956719817767654\n",
      "Recall 1.0\n",
      "Confusion Matrix [[4352   19]\n",
      " [   0 4371]]\n"
     ]
    }
   ],
   "source": [
    "#Creating a test Data frame and training model with test data\n",
    "df_benign = pd.DataFrame(X_test, columns=fs)\n",
    "df_benign['malicious'] = 0\n",
    "df_malicious = df_attack.sample(n=df_benign.shape[0], random_state=17)[fs]\n",
    "df_malicious['malicious'] = 1\n",
    "df2 = df_benign.append(df_malicious)\n",
    "X_test = df2.drop(columns=['malicious']).values\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "Y_test = df2['malicious']\n",
    "    \n",
    "#using best n value for treshold determined using opotimization dataset\n",
    "tr = mean + best_n * std\n",
    "m = AnomalyModel(model , tr, scaler)\n",
    "Y_pred = m.predict(X_test_scaled)\n",
    "print('Test-----------------')\n",
    "print(f'best n {best_n}')\n",
    "print(\"Threshold \",tr)\n",
    "uc_acc = accuracy_score(Y_test, Y_pred)\n",
    "uc_precision = precision_score(Y_test, Y_pred)\n",
    "uc_recall = recall_score(Y_test, Y_pred)\n",
    "uc_cm = confusion_matrix(Y_test, Y_pred)\n",
    "print('Accuracy ',uc_acc)\n",
    "print('Precision ',uc_precision)\n",
    "print(\"Recall\",uc_recall)\n",
    "print('Confusion Matrix',uc_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_model(input_dim):\n",
    "    input_img = Input(shape=(input_dim,))\n",
    "    encoded = Dense(200, activation='relu',\n",
    "                activity_regularizer=regularizers.l1(10e-6))(input_img)\n",
    "    decoded = Dense(input_dim, activation='sigmoid')(encoded)\n",
    "\n",
    "    autoencoder = Model(input_img, decoded)\n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "137/137 [==============================] - 2s 7ms/step - loss: 0.9102 - val_loss: 0.6902\n",
      "Epoch 2/100\n",
      "137/137 [==============================] - 0s 4ms/step - loss: 0.6765 - val_loss: 0.6702\n",
      "Epoch 3/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.6651 - val_loss: 0.6626\n",
      "Epoch 4/100\n",
      "137/137 [==============================] - 1s 6ms/step - loss: 0.6432 - val_loss: 0.6586\n",
      "Epoch 5/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.6776 - val_loss: 0.6568\n",
      "Epoch 6/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.6437 - val_loss: 0.6555\n",
      "Epoch 7/100\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.6780 - val_loss: 0.6547\n",
      "Epoch 8/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.6646 - val_loss: 0.6539\n",
      "Epoch 9/100\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.6271 - val_loss: 0.6534\n",
      "Epoch 10/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.6195 - val_loss: 0.6530\n",
      "Epoch 11/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.6443 - val_loss: 0.6526\n",
      "Epoch 12/100\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.6498 - val_loss: 0.6522\n",
      "Epoch 13/100\n",
      "137/137 [==============================] - 1s 6ms/step - loss: 0.6500 - val_loss: 0.6519\n",
      "Epoch 14/100\n",
      "137/137 [==============================] - 1s 6ms/step - loss: 0.6127 - val_loss: 0.6517\n",
      "Epoch 15/100\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.6257 - val_loss: 0.6515\n",
      "Epoch 16/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.6390 - val_loss: 0.6512\n",
      "Epoch 17/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.6714 - val_loss: 0.6511\n",
      "Epoch 18/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.6392 - val_loss: 0.6509\n",
      "Epoch 19/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.6473 - val_loss: 0.6507\n",
      "Epoch 20/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.6582 - val_loss: 0.6506\n",
      "Epoch 21/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.6416 - val_loss: 0.6504\n",
      "Epoch 22/100\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.6440 - val_loss: 0.6502\n",
      "Epoch 23/100\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.6552 - val_loss: 0.6501\n",
      "Epoch 24/100\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.6333 - val_loss: 0.6500\n",
      "Epoch 25/100\n",
      "137/137 [==============================] - 1s 7ms/step - loss: 0.6399 - val_loss: 0.6500\n",
      "Epoch 26/100\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.6602 - val_loss: 0.6499\n",
      "Epoch 27/100\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.6425 - val_loss: 0.6498\n",
      "Epoch 28/100\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.6359 - val_loss: 0.6497\n",
      "Epoch 29/100\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.6406 - val_loss: 0.6496\n",
      "Epoch 30/100\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.6737 - val_loss: 0.6495\n",
      "Epoch 31/100\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.6822 - val_loss: 0.6495\n",
      "Epoch 32/100\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.6668 - val_loss: 0.6494\n",
      "Epoch 33/100\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.6267 - val_loss: 0.6493\n",
      "Epoch 34/100\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.6470 - val_loss: 0.6493\n",
      "Epoch 35/100\n",
      "137/137 [==============================] - 1s 6ms/step - loss: 0.6414 - val_loss: 0.6494\n",
      "Epoch 36/100\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.6370 - val_loss: 0.6492\n",
      "Epoch 37/100\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.6463 - val_loss: 0.6492\n",
      "Epoch 38/100\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.6332 - val_loss: 0.6490\n",
      "Epoch 39/100\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.6225 - val_loss: 0.6492\n",
      "Epoch 40/100\n",
      "137/137 [==============================] - 1s 6ms/step - loss: 0.6472 - val_loss: 0.6490\n",
      "Epoch 41/100\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.6805 - val_loss: 0.6489\n",
      "Epoch 42/100\n",
      "137/137 [==============================] - 1s 6ms/step - loss: 0.6392 - val_loss: 0.6489\n",
      "Epoch 43/100\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.6456 - val_loss: 0.6487\n",
      "Epoch 44/100\n",
      "137/137 [==============================] - 1s 6ms/step - loss: 0.6651 - val_loss: 0.6488\n",
      "Epoch 45/100\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.6672 - val_loss: 0.6488\n",
      "Epoch 46/100\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.6351 - val_loss: 0.6488\n",
      "Epoch 47/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.6519 - val_loss: 0.6488\n",
      "Epoch 48/100\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.6479 - val_loss: 0.6487\n",
      "Epoch 49/100\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.6949 - val_loss: 0.6485\n",
      "Epoch 50/100\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.6184 - val_loss: 0.6485\n",
      "Epoch 51/100\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.6256 - val_loss: 0.6487\n",
      "Epoch 52/100\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.6657 - val_loss: 0.6488\n",
      "Epoch 53/100\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.6630 - val_loss: 0.6485\n",
      "Epoch 54/100\n",
      "137/137 [==============================] - 1s 6ms/step - loss: 0.6361 - val_loss: 0.6487\n",
      "Epoch 55/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.6699 - val_loss: 0.6486\n",
      "time: 38.89050555229187\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 115)]             0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 200)               23200     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 115)               23115     \n",
      "=================================================================\n",
      "Total params: 46,315\n",
      "Trainable params: 46,315\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "top_n_features=115\n",
    "fs = [it['feature'] for it in scored[:top_n_features]]\n",
    "X_train = x_train[fs]\n",
    "X_opt = x_opt[fs]\n",
    "X_test = x_test[fs]\n",
    "    \n",
    "#standardize train and optimize data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_opt = scaler.transform(X_opt)\n",
    "    \n",
    "#call function to create the model\n",
    "model = sparse_model(top_n_features)\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=\"adam\")\n",
    "cp = ModelCheckpoint(filepath=f\"anomaly/anomaly_sparse.h5\",monitor='val_loss',\n",
    "                     save_best_only=True,save_weights_only=True,verbose=0)\n",
    "\n",
    "#Early Stopping function until val_loss is same for 5 epochs\n",
    "es = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "start = time.time()  #time function to determine time taken by model\n",
    "epochs = 100\n",
    "    \n",
    "#fit on train dataset\n",
    "history = model.fit(X_train, X_train,\n",
    "                    epochs=epochs, validation_data=(X_opt, X_opt),\n",
    "                    verbose=1, callbacks=[cp, es])\n",
    "    \n",
    "end = time.time()\n",
    "print('time:', end - start)\n",
    "print(model.summary()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting n------------------\n",
      "For n 1\n",
      "Accuracy\n",
      "0.9874170670327156\n",
      "Precision 0.9754519080562375\n",
      "Recall 1.0\n",
      "Confusion Matrix [[4261  110]\n",
      " [   0 4371]]\n",
      "For n 2\n",
      "Accuracy\n",
      "0.9929078014184397\n",
      "Precision 0.9862333559016023\n",
      "Recall 0.9997712194005949\n",
      "Confusion Matrix [[4310   61]\n",
      " [   1 4370]]\n",
      "For n 3\n",
      "Accuracy\n",
      "0.9956531686113017\n",
      "Precision 0.9916042659405492\n",
      "Recall 0.9997712194005949\n",
      "Confusion Matrix [[4334   37]\n",
      " [   1 4370]]\n",
      "For n 4\n",
      "Accuracy\n",
      "0.9963395104095173\n",
      "Precision 0.9929561463303794\n",
      "Recall 0.9997712194005949\n",
      "Confusion Matrix [[4340   31]\n",
      " [   1 4370]]\n",
      "For n 5\n",
      "Accuracy\n",
      "0.996682681308625\n",
      "Precision 0.9936334697589814\n",
      "Recall 0.9997712194005949\n",
      "Confusion Matrix [[4343   28]\n",
      " [   1 4370]]\n",
      "For n 6\n",
      "Accuracy\n",
      "0.9970258522077328\n",
      "Precision 0.9943117178612059\n",
      "Recall 0.9997712194005949\n",
      "Confusion Matrix [[4346   25]\n",
      " [   1 4370]]\n",
      "For n 7\n",
      "Accuracy\n",
      "0.9973690231068405\n",
      "Precision 0.9949908925318761\n",
      "Recall 0.9997712194005949\n",
      "Confusion Matrix [[4349   22]\n",
      " [   1 4370]]\n",
      "For n 8\n",
      "Accuracy\n",
      "0.9980553649050561\n",
      "Precision 0.9965784671532847\n",
      "Recall 0.9995424388011896\n",
      "Confusion Matrix [[4356   15]\n",
      " [   2 4369]]\n",
      "For n 9\n",
      "Accuracy\n",
      "0.8763440860215054\n",
      "Precision 0.9969788519637462\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4361   10]\n",
      " [1071 3300]]\n",
      "For n 10\n",
      "Accuracy\n",
      "0.876458476321208\n",
      "Precision 0.9972801450589301\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4362    9]\n",
      " [1071 3300]]\n",
      "For n 11\n",
      "Accuracy\n",
      "0.876458476321208\n",
      "Precision 0.9972801450589301\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4362    9]\n",
      " [1071 3300]]\n",
      "For n 12\n",
      "Accuracy\n",
      "0.8765728666209105\n",
      "Precision 0.9975816203143894\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4363    8]\n",
      " [1071 3300]]\n",
      "For n 13\n",
      "Accuracy\n",
      "0.8768016472203157\n",
      "Precision 0.9981851179673321\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4365    6]\n",
      " [1071 3300]]\n",
      "For n 14\n",
      "Accuracy\n",
      "0.8770304278197208\n",
      "Precision 0.9987893462469734\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4367    4]\n",
      " [1071 3300]]\n",
      "For n 15\n",
      "Accuracy\n",
      "0.8770304278197208\n",
      "Precision 0.9987893462469734\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4367    4]\n",
      " [1071 3300]]\n",
      "For n 16\n",
      "Accuracy\n",
      "0.8770304278197208\n",
      "Precision 0.9987893462469734\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4367    4]\n",
      " [1071 3300]]\n",
      "For n 17\n",
      "Accuracy\n",
      "0.8770304278197208\n",
      "Precision 0.9987893462469734\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4367    4]\n",
      " [1071 3300]]\n",
      "For n 18\n",
      "Accuracy\n",
      "0.8770304278197208\n",
      "Precision 0.9987893462469734\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4367    4]\n",
      " [1071 3300]]\n",
      "For n 19\n",
      "Accuracy\n",
      "0.8771448181194235\n",
      "Precision 0.9990917347865577\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4368    3]\n",
      " [1071 3300]]\n",
      "For n 20\n",
      "Accuracy\n",
      "0.8772592084191261\n",
      "Precision 0.9993943064809206\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4369    2]\n",
      " [1071 3300]]\n",
      "For n 21\n",
      "Accuracy\n",
      "0.8772592084191261\n",
      "Precision 0.9993943064809206\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4369    2]\n",
      " [1071 3300]]\n",
      "For n 22\n",
      "Accuracy\n",
      "0.8772592084191261\n",
      "Precision 0.9993943064809206\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4369    2]\n",
      " [1071 3300]]\n",
      "For n 23\n",
      "Accuracy\n",
      "0.8772592084191261\n",
      "Precision 0.9993943064809206\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4369    2]\n",
      " [1071 3300]]\n",
      "For n 24\n",
      "Accuracy\n",
      "0.8774879890185312\n",
      "Precision 1.0\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4371    0]\n",
      " [1071 3300]]\n",
      "For n 25\n",
      "Accuracy\n",
      "0.8774879890185312\n",
      "Precision 1.0\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4371    0]\n",
      " [1071 3300]]\n",
      "For n 26\n",
      "Accuracy\n",
      "0.8774879890185312\n",
      "Precision 1.0\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4371    0]\n",
      " [1071 3300]]\n",
      "For n 27\n",
      "Accuracy\n",
      "0.8774879890185312\n",
      "Precision 1.0\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4371    0]\n",
      " [1071 3300]]\n",
      "For n 28\n",
      "Accuracy\n",
      "0.8774879890185312\n",
      "Precision 1.0\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4371    0]\n",
      " [1071 3300]]\n",
      "For n 29\n",
      "Accuracy\n",
      "0.8774879890185312\n",
      "Precision 1.0\n",
      "Recall 0.7549759780370625\n",
      "Confusion Matrix [[4371    0]\n",
      " [1071 3300]]\n"
     ]
    }
   ],
   "source": [
    "#use optimization dataset to predict and find mse for anomaly detection\n",
    "x_opt_predictions = model.predict(X_opt)\n",
    "mse = np.mean(np.power(X_opt - x_opt_predictions, 2), axis=1)\n",
    "mean = mse.mean()\n",
    "std = mse.std()\n",
    "    \n",
    "#sampling a benign dataset and adding a class malicious with 0\n",
    "df_benign = pd.DataFrame(x_opt[fs], columns=fs)\n",
    "df_benign['malicious'] = 0\n",
    "    \n",
    "#sampling a malicious dataset and adding a class malicious with 1\n",
    "df_malicious = df_attack.sample(n=df_benign.shape[0], random_state=39)[fs]\n",
    "df_malicious['malicious'] = 1\n",
    "    \n",
    "#combining benign and malicous dataset\n",
    "df_combo = df_benign.append(df_malicious)\n",
    "    \n",
    "#X_opt with feature values\n",
    "X_opt = df_combo.drop(columns=['malicious']).values\n",
    "X_opt_scaled = scaler.transform(X_opt)\n",
    "\n",
    "#Y_opt with target values\n",
    "Y_opt = df_combo['malicious']\n",
    "best_acc = 0\n",
    "best_n = 0\n",
    "#varying treshold and selecting the best model\n",
    "print('Selecting n------------------')\n",
    "for n in range(1,30):\n",
    "    tr = mean + n * std\n",
    "    m = AnomalyModel(model , tr, scaler)\n",
    "    Y_pred = m.predict(X_opt_scaled)\n",
    "    print(f'For n {n}')\n",
    "    print('Accuracy')\n",
    "    acc = accuracy_score(Y_opt, Y_pred)\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc  #determining the best aaccuracy\n",
    "        best_n = n\n",
    "    print(acc)\n",
    "    print('Precision',precision_score(Y_opt, Y_pred))\n",
    "    print(\"Recall\",recall_score(Y_opt, Y_pred))\n",
    "    print('Confusion Matrix',confusion_matrix(Y_opt, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test-----------------\n",
      "best n 8\n",
      "Threshold  15.693549062077409\n",
      "Accuracy  0.9985129261038664\n",
      "Precision  0.9970346715328468\n",
      "Recall 1.0\n",
      "Confusion Matrix [[4358   13]\n",
      " [   0 4371]]\n"
     ]
    }
   ],
   "source": [
    "#Creating a test Data frame and training model with test data\n",
    "df_benign = pd.DataFrame(X_test, columns=fs)\n",
    "df_benign['malicious'] = 0\n",
    "df_malicious = df_attack.sample(n=df_benign.shape[0], random_state=17)[fs]\n",
    "df_malicious['malicious'] = 1\n",
    "df2 = df_benign.append(df_malicious)\n",
    "X_test = df2.drop(columns=['malicious']).values\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "Y_test = df2['malicious']\n",
    "    \n",
    "#using best n value for treshold determined using opotimization dataset\n",
    "tr = mean + best_n * std\n",
    "m = AnomalyModel(model , tr, scaler)\n",
    "Y_pred = m.predict(X_test_scaled)\n",
    "print('Test-----------------')\n",
    "print(f'best n {best_n}')\n",
    "print(\"Threshold \",tr)\n",
    "sparse_acc = accuracy_score(Y_test, Y_pred)\n",
    "sparse_precision = precision_score(Y_test, Y_pred)\n",
    "sparse_recall = recall_score(Y_test, Y_pred)\n",
    "sparse_cm = confusion_matrix(Y_test, Y_pred)\n",
    "print('Accuracy ',sparse_acc)\n",
    "print('Precision ',sparse_precision)\n",
    "print(\"Recall\",sparse_recall)\n",
    "print('Confusion Matrix',sparse_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Denoising Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoise_model(input_dim):\n",
    "    input_size = input_dim\n",
    "    hidden_size = 44\n",
    "    code_size = 5\n",
    "\n",
    "    input_img = Input(shape=(input_size,))\n",
    "    hidden_1 = Dense(hidden_size, activation='relu')(input_img)\n",
    "    code = Dense(code_size, activation='relu')(hidden_1)\n",
    "    hidden_2 = Dense(hidden_size, activation='relu')(code)\n",
    "    output_img = Dense(input_size, activation='sigmoid')(hidden_2)\n",
    "\n",
    "    autoencoder = Model(input_img, output_img)\n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "137/137 [==============================] - 2s 8ms/step - loss: 1.1504 - val_loss: 1.0035\n",
      "Epoch 2/100\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.9991 - val_loss: 0.8401\n",
      "Epoch 3/100\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.8275 - val_loss: 0.7957\n",
      "Epoch 4/100\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.8269 - val_loss: 0.7925\n",
      "Epoch 5/100\n",
      "137/137 [==============================] - 1s 6ms/step - loss: 0.8275 - val_loss: 0.7900\n",
      "Epoch 6/100\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.7559 - val_loss: 0.7885\n",
      "Epoch 7/100\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.7595 - val_loss: 0.7869\n",
      "Epoch 8/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.8100 - val_loss: 0.7851\n",
      "Epoch 9/100\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.7849 - val_loss: 0.7822\n",
      "Epoch 10/100\n",
      "137/137 [==============================] - 1s 6ms/step - loss: 0.7520 - val_loss: 0.7792\n",
      "Epoch 11/100\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.8253 - val_loss: 0.7783\n",
      "Epoch 12/100\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.7646 - val_loss: 0.7737\n",
      "Epoch 13/100\n",
      "137/137 [==============================] - 1s 6ms/step - loss: 0.7776 - val_loss: 0.7721\n",
      "Epoch 14/100\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.7701 - val_loss: 0.7718\n",
      "Epoch 15/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.7798 - val_loss: 0.7687\n",
      "Epoch 16/100\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.7294 - val_loss: 0.7814\n",
      "Epoch 17/100\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.7833 - val_loss: 0.7669\n",
      "Epoch 18/100\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.7477 - val_loss: 0.7643\n",
      "Epoch 19/100\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.7569 - val_loss: 0.7636\n",
      "Epoch 20/100\n",
      "137/137 [==============================] - 1s 6ms/step - loss: 0.7770 - val_loss: 0.7681\n",
      "Epoch 21/100\n",
      "137/137 [==============================] - 1s 8ms/step - loss: 0.7742 - val_loss: 0.7658\n",
      "Epoch 22/100\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.7598 - val_loss: 0.7615\n",
      "Epoch 23/100\n",
      "137/137 [==============================] - 0s 4ms/step - loss: 0.7136 - val_loss: 0.7609\n",
      "Epoch 24/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.7569 - val_loss: 0.7635\n",
      "Epoch 25/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.7395 - val_loss: 0.7598\n",
      "Epoch 26/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.7320 - val_loss: 0.7645\n",
      "Epoch 27/100\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.7490 - val_loss: 0.7567\n",
      "Epoch 28/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.7343 - val_loss: 0.7568\n",
      "Epoch 29/100\n",
      "137/137 [==============================] - 0s 4ms/step - loss: 0.7676 - val_loss: 0.7557\n",
      "Epoch 30/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.7192 - val_loss: 0.7552\n",
      "Epoch 31/100\n",
      "137/137 [==============================] - 1s 6ms/step - loss: 0.7141 - val_loss: 0.7587\n",
      "Epoch 32/100\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.7522 - val_loss: 0.7558\n",
      "Epoch 33/100\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.7389 - val_loss: 0.7553\n",
      "Epoch 34/100\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.7714 - val_loss: 0.7551\n",
      "Epoch 35/100\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.7638 - val_loss: 0.7551\n",
      "Epoch 36/100\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.7451 - val_loss: 0.7557\n",
      "Epoch 37/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.7734 - val_loss: 0.7558\n",
      "Epoch 38/100\n",
      "137/137 [==============================] - 1s 8ms/step - loss: 0.7430 - val_loss: 0.7543\n",
      "Epoch 39/100\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.7442 - val_loss: 0.7562\n",
      "Epoch 40/100\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.7434 - val_loss: 0.7536\n",
      "Epoch 41/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.7608 - val_loss: 0.7524\n",
      "Epoch 42/100\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.7497 - val_loss: 0.7596\n",
      "Epoch 43/100\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.7265 - val_loss: 0.7530\n",
      "Epoch 44/100\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.7037 - val_loss: 0.7523\n",
      "Epoch 45/100\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.7214 - val_loss: 0.7515\n",
      "Epoch 46/100\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.7572 - val_loss: 0.7526\n",
      "Epoch 47/100\n",
      "137/137 [==============================] - 0s 4ms/step - loss: 0.7336 - val_loss: 0.7517\n",
      "Epoch 48/100\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.7379 - val_loss: 0.7519\n",
      "Epoch 49/100\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.7892 - val_loss: 0.7536\n",
      "Epoch 50/100\n",
      "137/137 [==============================] - 0s 3ms/step - loss: 0.7692 - val_loss: 0.7544\n",
      "time: 33.49145269393921\n",
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, 115)]             0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 44)                5104      \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 5)                 225       \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 44)                264       \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 115)               5175      \n",
      "=================================================================\n",
      "Total params: 10,768\n",
      "Trainable params: 10,768\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "top_n_features=115\n",
    "fs = [it['feature'] for it in scored[:top_n_features]]\n",
    "X_train = x_train[fs]\n",
    "X_opt = x_opt[fs]\n",
    "X_test = x_test[fs]\n",
    "    \n",
    "#standardize train and optimize data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "noise_factor = 0.5\n",
    "x_train_noisy = X_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=X_train.shape) \n",
    "x_test_noisy = X_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=X_test.shape) \n",
    "x_opt_noisy = X_opt + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=X_opt.shape) \n",
    "x_train_noisy = np.clip(x_train_noisy, 0., 1.)\n",
    "x_test_noisy = np.clip(x_test_noisy, 0., 1.)\n",
    "x_opt_noisy = np.clip(x_opt_noisy, 0., 1.)\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_opt = scaler.transform(X_opt)\n",
    "#x_train_noisy = scaler.fit_transform(x_train_noisy)\n",
    "#x_opt_noisy = scaler.transform(x_opt_noisy)\n",
    "\n",
    "#call function to create the model\n",
    "model = denoise_model(top_n_features)\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=\"adam\")\n",
    "cp = ModelCheckpoint(filepath=f\"anomaly/anomaly_sparse.h5\",monitor='val_loss',\n",
    "                     save_best_only=True,save_weights_only=True,verbose=0)\n",
    "\n",
    "#Early Stopping function until val_loss is same for 5 epochs\n",
    "es = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "start = time.time()  #time function to determine time taken by model\n",
    "epochs = 100\n",
    "    \n",
    "#fit on train dataset\n",
    "history = model.fit(x_train_noisy, X_train,\n",
    "                    epochs=epochs, validation_data=(x_opt_noisy, X_opt),\n",
    "                    verbose=1, callbacks=[cp, es])\n",
    "    \n",
    "end = time.time()\n",
    "print('time:', end - start)\n",
    "print(model.summary()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting n------------------\n",
      "For n 1\n",
      "Accuracy\n",
      "0.5001143902997026\n",
      "Precision 0.5000572016931701\n",
      "Recall 1.0\n",
      "Confusion Matrix [[   1 4370]\n",
      " [   0 4371]]\n",
      "For n 2\n",
      "Accuracy\n",
      "0.5009151223976207\n",
      "Precision 0.5004579803068469\n",
      "Recall 1.0\n",
      "Confusion Matrix [[   8 4363]\n",
      " [   0 4371]]\n",
      "For n 3\n",
      "Accuracy\n",
      "0.5022878059940517\n",
      "Precision 0.5011465260261408\n",
      "Recall 1.0\n",
      "Confusion Matrix [[  20 4351]\n",
      " [   0 4371]]\n",
      "For n 4\n",
      "Accuracy\n",
      "0.5082361015785861\n",
      "Precision 0.504152249134948\n",
      "Recall 1.0\n",
      "Confusion Matrix [[  72 4299]\n",
      " [   0 4371]]\n",
      "For n 5\n",
      "Accuracy\n",
      "0.5161290322580645\n",
      "Precision 0.5081967213114754\n",
      "Recall 1.0\n",
      "Confusion Matrix [[ 141 4230]\n",
      " [   0 4371]]\n",
      "For n 6\n",
      "Accuracy\n",
      "0.5277968428277282\n",
      "Precision 0.5142957995058243\n",
      "Recall 1.0\n",
      "Confusion Matrix [[ 243 4128]\n",
      " [   0 4371]]\n",
      "For n 7\n",
      "Accuracy\n",
      "0.543010752688172\n",
      "Precision 0.5224719101123596\n",
      "Recall 1.0\n",
      "Confusion Matrix [[ 376 3995]\n",
      " [   0 4371]]\n",
      "For n 8\n",
      "Accuracy\n",
      "0.5626858842370167\n",
      "Precision 0.5334391017817915\n",
      "Recall 1.0\n",
      "Confusion Matrix [[ 548 3823]\n",
      " [   0 4371]]\n",
      "For n 9\n",
      "Accuracy\n",
      "0.5863646762754519\n",
      "Precision 0.547264304494804\n",
      "Recall 1.0\n",
      "Confusion Matrix [[ 755 3616]\n",
      " [   0 4371]]\n",
      "For n 10\n",
      "Accuracy\n",
      "0.612788835506749\n",
      "Precision 0.5635636926250644\n",
      "Recall 1.0\n",
      "Confusion Matrix [[ 986 3385]\n",
      " [   0 4371]]\n",
      "For n 11\n",
      "Accuracy\n",
      "0.6433310455273393\n",
      "Precision 0.583656028842302\n",
      "Recall 1.0\n",
      "Confusion Matrix [[1253 3118]\n",
      " [   0 4371]]\n",
      "For n 12\n",
      "Accuracy\n",
      "0.675703500343171\n",
      "Precision 0.6065778517901749\n",
      "Recall 1.0\n",
      "Confusion Matrix [[1536 2835]\n",
      " [   0 4371]]\n",
      "For n 13\n",
      "Accuracy\n",
      "0.7020132692747655\n",
      "Precision 0.6265768348623854\n",
      "Recall 1.0\n",
      "Confusion Matrix [[1766 2605]\n",
      " [   0 4371]]\n",
      "For n 14\n",
      "Accuracy\n",
      "0.7296957218027911\n",
      "Precision 0.6490941490941491\n",
      "Recall 1.0\n",
      "Confusion Matrix [[2008 2363]\n",
      " [   0 4371]]\n",
      "For n 15\n",
      "Accuracy\n",
      "0.7606954930221917\n",
      "Precision 0.6763113105369024\n",
      "Recall 1.0\n",
      "Confusion Matrix [[2279 2092]\n",
      " [   0 4371]]\n",
      "For n 16\n",
      "Accuracy\n",
      "0.7910089224433768\n",
      "Precision 0.7052274927395934\n",
      "Recall 1.0\n",
      "Confusion Matrix [[2544 1827]\n",
      " [   0 4371]]\n",
      "For n 17\n",
      "Accuracy\n",
      "0.8143445435827041\n",
      "Precision 0.7292292292292293\n",
      "Recall 1.0\n",
      "Confusion Matrix [[2748 1623]\n",
      " [   0 4371]]\n",
      "For n 18\n",
      "Accuracy\n",
      "0.8383665065202471\n",
      "Precision 0.7557053941908713\n",
      "Recall 1.0\n",
      "Confusion Matrix [[2958 1413]\n",
      " [   0 4371]]\n",
      "For n 19\n",
      "Accuracy\n",
      "0.8565545641729582\n",
      "Precision 0.7770666666666667\n",
      "Recall 1.0\n",
      "Confusion Matrix [[3117 1254]\n",
      " [   0 4371]]\n",
      "For n 20\n",
      "Accuracy\n",
      "0.875314573324182\n",
      "Precision 0.8004028566196667\n",
      "Recall 1.0\n",
      "Confusion Matrix [[3281 1090]\n",
      " [   0 4371]]\n",
      "For n 21\n",
      "Accuracy\n",
      "0.8907572637840311\n",
      "Precision 0.8206909500563274\n",
      "Recall 1.0\n",
      "Confusion Matrix [[3416  955]\n",
      " [   0 4371]]\n",
      "For n 22\n",
      "Accuracy\n",
      "0.905971173644475\n",
      "Precision 0.8417099942229925\n",
      "Recall 1.0\n",
      "Confusion Matrix [[3549  822]\n",
      " [   0 4371]]\n",
      "For n 23\n",
      "Accuracy\n",
      "0.918897277510867\n",
      "Precision 0.8604330708661417\n",
      "Recall 1.0\n",
      "Confusion Matrix [[3662  709]\n",
      " [   0 4371]]\n",
      "For n 24\n",
      "Accuracy\n",
      "0.9299931365820179\n",
      "Precision 0.8771824202287778\n",
      "Recall 1.0\n",
      "Confusion Matrix [[3759  612]\n",
      " [   0 4371]]\n",
      "For n 25\n",
      "Accuracy\n",
      "0.9389155799588195\n",
      "Precision 0.891131498470948\n",
      "Recall 1.0\n",
      "Confusion Matrix [[3837  534]\n",
      " [   0 4371]]\n",
      "For n 26\n",
      "Accuracy\n",
      "0.9471516815374056\n",
      "Precision 0.904407200496586\n",
      "Recall 1.0\n",
      "Confusion Matrix [[3909  462]\n",
      " [   0 4371]]\n",
      "For n 27\n",
      "Accuracy\n",
      "0.9548158316174788\n",
      "Precision 0.9171212757028955\n",
      "Recall 1.0\n",
      "Confusion Matrix [[3976  395]\n",
      " [   0 4371]]\n",
      "For n 28\n",
      "Accuracy\n",
      "0.9608785175017158\n",
      "Precision 0.9274347549331636\n",
      "Recall 1.0\n",
      "Confusion Matrix [[4029  342]\n",
      " [   0 4371]]\n",
      "For n 29\n",
      "Accuracy\n",
      "0.9656829100892245\n",
      "Precision 0.9357739242132306\n",
      "Recall 1.0\n",
      "Confusion Matrix [[4071  300]\n",
      " [   0 4371]]\n"
     ]
    }
   ],
   "source": [
    "#use optimization dataset to predict and find mse for anomaly detection\n",
    "x_opt_predictions = model.predict(x_opt_noisy)\n",
    "mse = np.mean(np.power(x_opt_noisy - x_opt_predictions, 2), axis=1)\n",
    "mean = mse.mean()\n",
    "std = mse.std()\n",
    "    \n",
    "#sampling a benign dataset and adding a class malicious with 0\n",
    "df_benign = pd.DataFrame(x_opt[fs], columns=fs)\n",
    "df_benign['malicious'] = 0\n",
    "    \n",
    "#sampling a malicious dataset and adding a class malicious with 1\n",
    "df_malicious = df_attack.sample(n=df_benign.shape[0], random_state=39)[fs]\n",
    "df_malicious['malicious'] = 1\n",
    "    \n",
    "#combining benign and malicous dataset\n",
    "df_combo = df_benign.append(df_malicious)\n",
    "    \n",
    "#X_opt with feature values\n",
    "X_opt = df_combo.drop(columns=['malicious']).values\n",
    "x_opt_noisy = X_opt + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=X_opt.shape) \n",
    "X_opt_scaled = scaler.transform(x_opt_noisy)\n",
    "\n",
    "#Y_opt with target values\n",
    "Y_opt = df_combo['malicious']\n",
    "best_acc = 0\n",
    "best_n = 0\n",
    "#varying treshold and selecting the best model\n",
    "print('Selecting n------------------')\n",
    "for n in range(1,30):\n",
    "    tr = mean + n * std\n",
    "    m = AnomalyModel(model , tr, scaler)\n",
    "    Y_pred = m.predict(X_opt_scaled)\n",
    "    print(f'For n {n}')\n",
    "    print('Accuracy')\n",
    "    acc = accuracy_score(Y_opt, Y_pred)\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc  #determining the best aaccuracy\n",
    "        best_n = n\n",
    "    print(acc)\n",
    "    print('Precision',precision_score(Y_opt, Y_pred))\n",
    "    print(\"Recall\",recall_score(Y_opt, Y_pred))\n",
    "    print('Confusion Matrix',confusion_matrix(Y_opt, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test-----------------\n",
      "best n 29\n",
      "Threshold  3.922659487657792\n",
      "Accuracy  0.5235644017387325\n",
      "Precision  0.512066541705717\n",
      "Recall 1.0\n",
      "Confusion Matrix [[ 206 4165]\n",
      " [   0 4371]]\n"
     ]
    }
   ],
   "source": [
    "#Creating a test Data frame and training model with test data\n",
    "df_benign = pd.DataFrame(x_test_noisy, columns=fs)\n",
    "df_benign['malicious'] = 0\n",
    "df_malicious = df_attack.sample(n=df_benign.shape[0], random_state=20)[fs]\n",
    "df_malicious['malicious'] = 1\n",
    "df2 = df_benign.append(df_malicious)\n",
    "X_test = df2.drop(columns=['malicious']).values\n",
    "x_test_noisy = X_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=X_test.shape) \n",
    "X_test_scaled = scaler.transform(x_test_noisy)\n",
    "Y_test = df2['malicious']\n",
    "    \n",
    "#using best n value for treshold determined using opotimization dataset\n",
    "tr = mean + best_n * std\n",
    "m = AnomalyModel(model , tr, scaler)\n",
    "Y_pred = m.predict(X_test_scaled)\n",
    "print('Test-----------------')\n",
    "print(f'best n {best_n}')\n",
    "print(\"Threshold \",tr)\n",
    "denoise_acc = accuracy_score(Y_test, Y_pred)\n",
    "denoise_precision = precision_score(Y_test, Y_pred)\n",
    "denoise_recall = recall_score(Y_test, Y_pred)\n",
    "denoise_cm = confusion_matrix(Y_test, Y_pred)\n",
    "print('Accuracy ',denoise_acc)\n",
    "print('Precision ',denoise_precision)\n",
    "print(\"Recall\",denoise_recall)\n",
    "print('Confusion Matrix',denoise_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dnn_model(inputs):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(115, input_dim=inputs, activation='relu'))\n",
    "    model.add(Dense(80, input_dim=inputs, activation='relu'))\n",
    "    model.add(Dense(10, input_dim=inputs, activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    model.add(Dense(labels.shape[1],activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    es = EarlyStopping(monitor='val_loss', min_delta=1e-3, callbacks=[monitor], patience=5) \n",
    "    model.fit(x_train_st,y_train_st,validation_data=(x_test_st,y_test_st),\n",
    "          verbose=1,epochs=100, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_attack = df_mirai.append(df_gafgyt)\n",
    "df_attack['class'] = 1\n",
    "\n",
    "df_ben = x_train.copy()\n",
    "df_ben['class'] = 0\n",
    "\n",
    "df_ben = df_ben.append(df_attack.sample(n=df_ben.shape[0], random_state=17))\n",
    "\n",
    "X = df_ben.drop(columns=['class'])\n",
    "y = df_ben['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split train and test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2,random_state=17)\n",
    "\n",
    "#scale data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "467/467 [==============================] - 2s 2ms/step - loss: 0.0115\n",
      "Epoch 2/100\n",
      "467/467 [==============================] - 1s 2ms/step - loss: 7.1540e-04\n",
      "Epoch 3/100\n",
      "467/467 [==============================] - 1s 2ms/step - loss: 7.4876e-04\n",
      "Epoch 4/100\n",
      "467/467 [==============================] - 1s 2ms/step - loss: 4.4497e-04\n",
      "Epoch 5/100\n",
      "467/467 [==============================] - 1s 2ms/step - loss: 0.0016\n",
      "Epoch 6/100\n",
      "467/467 [==============================] - 1s 2ms/step - loss: 8.4631e-04\n",
      "Epoch 7/100\n",
      "467/467 [==============================] - 1s 2ms/step - loss: 2.0603e-04\n",
      "Epoch 8/100\n",
      "467/467 [==============================] - 1s 2ms/step - loss: 7.6586e-04\n",
      "Epoch 9/100\n",
      "467/467 [==============================] - 1s 2ms/step - loss: 9.3373e-05\n",
      "Epoch 10/100\n",
      "467/467 [==============================] - 1s 2ms/step - loss: 9.9690e-04\n",
      "Epoch 11/100\n",
      "467/467 [==============================] - 1s 2ms/step - loss: 5.0171e-05\n",
      "Epoch 12/100\n",
      "467/467 [==============================] - 1s 2ms/step - loss: 2.1402e-04\n",
      "Epoch 13/100\n",
      "467/467 [==============================] - 1s 2ms/step - loss: 0.0015\n",
      "Epoch 14/100\n",
      "467/467 [==============================] - 1s 2ms/step - loss: 2.9857e-04\n",
      "Epoch 15/100\n",
      "467/467 [==============================] - 1s 2ms/step - loss: 3.0249e-04\n",
      "Epoch 16/100\n",
      "467/467 [==============================] - 1s 2ms/step - loss: 4.1058e-04\n",
      "Epoch 17/100\n",
      "467/467 [==============================] - 1s 2ms/step - loss: 3.1228e-04\n",
      "time 15.792693853378296\n"
     ]
    }
   ],
   "source": [
    "#create model\n",
    "model = Sequential()\n",
    "model.add(Dense(115, input_dim=115, activation='relu'))\n",
    "model.add(Dense(80, activation='relu'))\n",
    "model.add(Dense(30, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "es = EarlyStopping(monitor='loss', patience=5, restore_best_weights=True)\n",
    "start = time.time()  #time function to determine time taken by model\n",
    "epochs = 100\n",
    "\n",
    "#compile and fit model\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=\"adam\")\n",
    "model.fit(X_train,y_train,epochs=epochs,batch_size=15,callbacks=[es])\n",
    "\n",
    "end = time.time()\n",
    "print(\"time\",end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)\n",
    "y_pred = [round(x[0]) for x in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy  0.9948542024013722\n",
      "Precision  0.9965277777777778\n",
      "Recall 0.9930795847750865\n",
      "Confusion Matrix [[879   3]\n",
      " [  6 861]]\n"
     ]
    }
   ],
   "source": [
    "dnn_acc = accuracy_score(y_test, y_pred)\n",
    "dnn_precision = precision_score(y_test, y_pred)\n",
    "dnn_recall = recall_score(y_test, y_pred)\n",
    "dnn_cm = confusion_matrix(y_test, y_pred)\n",
    "print('Accuracy ',dnn_acc)\n",
    "print('Precision ',dnn_precision)\n",
    "print(\"Recall\",dnn_recall)\n",
    "print('Confusion Matrix',dnn_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thermostat\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>CM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Denoising Autoendoer</td>\n",
       "      <td>0.523564</td>\n",
       "      <td>0.512067</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>[[206, 4165], [0, 4371]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Deep AutoEncoder</td>\n",
       "      <td>0.880577</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.761153</td>\n",
       "      <td>[[4371, 0], [1044, 3327]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Undercomplete Autoencoder</td>\n",
       "      <td>0.997827</td>\n",
       "      <td>0.995672</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>[[4352, 19], [0, 4371]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Variational AutoEncoder</td>\n",
       "      <td>0.998399</td>\n",
       "      <td>0.996807</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>[[4357, 14], [0, 4371]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sparse AutoEncoder</td>\n",
       "      <td>0.998513</td>\n",
       "      <td>0.997035</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>[[4358, 13], [0, 4371]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Model  Accuracy  Precision    Recall  \\\n",
       "4       Denoising Autoendoer  0.523564   0.512067  1.000000   \n",
       "0           Deep AutoEncoder  0.880577   1.000000  0.761153   \n",
       "3  Undercomplete Autoencoder  0.997827   0.995672  1.000000   \n",
       "1    Variational AutoEncoder  0.998399   0.996807  1.000000   \n",
       "2         Sparse AutoEncoder  0.998513   0.997035  1.000000   \n",
       "\n",
       "                          CM  \n",
       "4   [[206, 4165], [0, 4371]]  \n",
       "0  [[4371, 0], [1044, 3327]]  \n",
       "3    [[4352, 19], [0, 4371]]  \n",
       "1    [[4357, 14], [0, 4371]]  \n",
       "2    [[4358, 13], [0, 4371]]  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Thermostat\")\n",
    "model_summary = pd.DataFrame({'Model' :['Deep AutoEncoder',\"Variational AutoEncoder\",\"Sparse AutoEncoder\",\"Undercomplete Autoencoder\",\"Denoising Autoendoer\"],\n",
    "                             'Accuracy':[deep_acc,vae_acc,sparse_acc,uc_acc,denoise_acc],\n",
    "                             'Precision':[deep_precision,vae_precision,sparse_precision,uc_precision,denoise_precision],\n",
    "                             'Recall':[deep_recall,vae_recall,sparse_recall,uc_recall,denoise_recall],\n",
    "                             'CM':[deep_cm,vae_cm,sparse_cm,uc_cm, denoise_cm]})\n",
    "model_summary.sort_values(by='Accuracy', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
